{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ddc8b8c-d63b-4018-a8e3-abefb71373bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchvision import models, transforms, datasets\n",
    "import torch\n",
    "from torch import nn\n",
    "from utils import myCNN\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9da497-98f2-45ef-8fd7-c3d5c690d3e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data_path = '/mnt/d/Dataset/asl-alphabet/asl_alphabet_train/'\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48370d39-70cc-4868-8f91-2f8a05dbf0fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize(32),\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.Resize(32),\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66dab6d8-0455-42e1-866a-9848dd6af9b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "train_dataset = datasets.ImageFolder(train_data_path, transform=train_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2727e2b-e9ad-490e-87c3-eec1fbc02e45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "val_dataset = datasets.ImageFolder(train_data_path, transform=test_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8648ddfa-16cf-4cfa-8ff8-04ab5db8c005",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "num_train_samples = len(train_dataset)\n",
    "# num_train_samples = 20000\n",
    "\n",
    "val_split = 0.2\n",
    "split = int(num_train_samples * val_split)\n",
    "indices = torch.randperm(num_train_samples)\n",
    "\n",
    "\n",
    "train_subset = torch.utils.data.Subset(train_dataset, indices[split:])\n",
    "val_subset = torch.utils.data.Subset(val_dataset, indices[:split])\n",
    "\n",
    "len(train_subset), len(val_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ee8bcc-405f-46fe-a70b-0acf7a580c8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    dataset=train_subset, \n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=12,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "    dataset=val_subset,\n",
    "    batch_size=4,\n",
    "    shuffle=False,\n",
    "    num_workers=12,\n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e481cf-072c-4fa6-bf93-44cb1ab26675",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "classes = train_dataloader.dataset.dataset.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7552c567-32bb-4f66-871f-4d84c1a1a8d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for img, label in train_dataloader:\n",
    "    print(img.shape, label.shape)\n",
    "    print(f'Ground Truth {classes[label[0]]}')\n",
    "    print(img[0].size())\n",
    "    print(img[0].permute(1, 2, 0).size())\n",
    "    plt.imshow(img[0].permute(1, 2, 0),cmap='gray')\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbba33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = myCNN().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b10360f",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, \"min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f99fea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train(model,\n",
    "          criterion,\n",
    "          optimizer,\n",
    "          train_dataloader,\n",
    "          test_dataloader,\n",
    "          print_every,\n",
    "          num_epoch):\n",
    "    steps = 0\n",
    "    train_losses, val_losses = [], []\n",
    "\n",
    "    model.to(device)\n",
    "    for epoch in tqdm(range(num_epoch)):\n",
    "        running_loss = 0\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "        start_time = time()\n",
    "        iter_time = time()\n",
    "        \n",
    "        #model.train()\n",
    "        for i, (images, labels) in enumerate(train_dataloader):\n",
    "            steps += 1\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            \n",
    "            output = model(images)\n",
    "            loss = criterion(output, labels)\n",
    "            \n",
    "            correct_train += (torch.max(output, dim=1)[1] == labels).type(torch.float).sum().item()\n",
    "            total_train += labels.size(0)\n",
    "            \n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # Logging\n",
    "            if steps % print_every == 0:\n",
    "                print(f'Epoch [{epoch + 1}]/[{num_epoch}]. Batch [{i + 1}]/[{len(train_dataloader)}].', end=' ')\n",
    "                print(f'Train loss {running_loss / steps:.5f}.', end=' ')\n",
    "                print(f'Train acc {correct_train / total_train * 100:.5f}.', end=' ')\n",
    "                with torch.no_grad():\n",
    "                    # model.eval()\n",
    "                    correct_val, total_val = 0, 0\n",
    "                    val_loss = 0\n",
    "                    for images, labels in test_dataloader:\n",
    "                        images = images.to(device)\n",
    "                        labels = labels.to(device)\n",
    "                        output = model(images)\n",
    "                        loss = criterion(output, labels)\n",
    "                        val_loss += loss.item()\n",
    "\n",
    "                        correct_val += (torch.max(output, dim=1)[1] == labels).type(torch.float).sum().item()\n",
    "                        total_val += labels.size(0)\n",
    "\n",
    "                print(f'Val loss {val_loss / len(test_dataloader):.5f}. Val acc {correct_val / total_val * 100:.5f}.', end=' ')\n",
    "                print(f'Took {time() - iter_time:.5f} seconds')\n",
    "                iter_time = time()\n",
    "\n",
    "                train_losses.append(running_loss / total_train)\n",
    "                val_losses.append(val_loss / total_val)\n",
    "        scheduler.step(val_loss / len(test_dataloader))\n",
    "\n",
    "\n",
    "        print(f'Epoch took {time() - start_time}') \n",
    "        torch.save(model, f'checkpoints/checkpoint_{correct_val / total_val * 100:.2f}.pth')\n",
    "        \n",
    "    return model, train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b251b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_every = 50\n",
    "num_epoch = 100\n",
    "\n",
    "model, train_losses, val_losses = train(\n",
    "    model=model,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    train_dataloader=train_dataloader,\n",
    "    test_dataloader=val_dataloader,\n",
    "    print_every=print_every,\n",
    "    num_epoch=num_epoch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2effd7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_losses, label='Training loss')\n",
    "plt.plot(val_losses, label='Validation loss')\n",
    "plt.legend(frameon=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475f101d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "test_data_path = Path('/mnt/d/Dataset/asl-alphabet/asl_alphabet_test')\n",
    "\n",
    "\n",
    "class ASLTestDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root_path, transforms=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.transforms = transforms\n",
    "        self.imgs = sorted(list(Path(root_path).glob('*.jpg')))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.imgs[idx]\n",
    "        img = Image.open(img_path).convert('L')\n",
    "        \n",
    "        label = img_path.parts[-1].split('_')[0]\n",
    "        if self.transforms:\n",
    "            img = self.transforms(img)\n",
    "        \n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364d043e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = ASLTestDataset(test_data_path, transforms=test_transforms)\n",
    "\n",
    "columns = 7\n",
    "row = round(len(test_dataset) / columns) + 1\n",
    "\n",
    "fig, ax = plt.subplots(row, columns, figsize=(columns * row, row * columns))\n",
    "plt.subplots_adjust(wspace=0.1, hspace=0.2)\n",
    "\n",
    "test_model = torch.load(\"checkpoints/checkpoint_98.53.pth\", map_location='cpu')\n",
    "#test_model = model\n",
    "test_model.to(device)\n",
    "\n",
    "i, j = 0, 0\n",
    "for img, label in test_dataset:\n",
    "    img = torch.Tensor(img)\n",
    "    img = img.to(device)\n",
    "    test_model.eval()\n",
    "    prediction = test_model(img[None])\n",
    "\n",
    "    ax[i][j].imshow(img.cpu().permute(1, 2, 0),cmap='gray')\n",
    "    ax[i][j].set_title(f'GT {label}. Pred {classes[torch.max(prediction, dim=1)[1]]}') #torch.max(prediction, dim=1)[1]\n",
    "    ax[i][j].axis('off')\n",
    "    j += 1\n",
    "    if j == columns:\n",
    "        j = 0\n",
    "        i += 1\n",
    "        \n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
